{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maheenms/GoogleCoLab/blob/main/nlp_hashingTF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKHacoX0qc-u",
        "outputId": "8a0ae90b-83b1-4be3-9e7d-22e5c6df1a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease [1,581 B]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:5 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [98.9 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [992 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [83.3 kB]\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:16 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,216 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,554 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,472 kB]\n",
            "Get:19 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,133 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [3,040 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [1,226 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [1,267 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,332 kB]\n",
            "Fetched 17.6 MB in 9s (1,875 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.2  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "# spark_version = 'spark-3.2.2'\n",
        "spark_version = 'spark-3.2.2'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "BZZ7EEmVw19M"
      },
      "outputs": [],
      "source": [
        "# Start Spark session\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Hashing\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "hb1lwcPJqV_y"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import HashingTF, IDF, Tokenizer # term frequency--> measure or count of a word that occurs  and Inverse document frequency--> evaluates the most imp words in the phrases"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF (term frequency-inverse document frequency) is an information retrieval technique that helps find the most relevant documents corresponding to a given query.\n",
        "\n",
        "TF is a measure of how often a phrase appears in a document, and IDF is about how important that phrase is. The multiplication of these two scores makes up a TF-IDF score."
      ],
      "metadata": {
        "id": "2B0Q9sRAEO14"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google has been using TF-IDF (or TF  IDF, TF*IDF, TFIDF, TF.IDF) to rank your content for a long time. It seems that Google focuses more on term frequency rather than on counting keywords. "
      ],
      "metadata": {
        "id": "NRKm1xa0EVgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF is used by search engines to better understand the content that is undervalued. For example, when you search for “Coke” on Google, Google may use TF-IDF to figure out if a page titled “COKE” is about:\n",
        "\n",
        "1. Coca-Cola.\n",
        "2. Drugs.\n",
        "3. A solid, carbon-rich residue derived from the distillation of crude oil.\n",
        "4. A county in Texas.\n"
      ],
      "metadata": {
        "id": "GdFK_jSjEXNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The TF-IDF algorithm is used to weigh a keyword in any content and assign importance to that keyword based on the number of times it appears in the document. More importantly, it checks how relevant the keyword is throughout the web, which is referred to as corpus.\n",
        "\n",
        "For a term t in document d, the weight Wt,d of term t in document d is given by:\n",
        "\n",
        "Wt,d = TFt,d log (N/DFt)\n",
        "\n",
        "Where:\n",
        "\n",
        "* TFt,d is the number of occurrences of t in document d.\n",
        "\n",
        "* DFt is the number of documents containing the term t.\n",
        "\n",
        "* N is the total number of documents in the corpus. (number of rows of data)"
      ],
      "metadata": {
        "id": "-VRnom3wEnQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How is the TF-IDF score calculated?**\n",
        "\n",
        "TF-IDF is scored between 0 and 1. The higher the numerical weight value, the rarer the term. The smaller the weight, the more common the term. "
      ],
      "metadata": {
        "id": "H68YJ9uyFIC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF (term frequency) example\n",
        "The TF (term frequency) of a word is the frequency of a word (i.e., number of times it appears) in a document. When you know TF, you’re able to see if you’re using a term too much or too little.\n",
        "\n",
        "When a 100-word document contains the term “cat” 12 times, the TF for the word ‘cat’ is\n",
        "\n",
        "TFcat = 12/100 i.e. 0.12"
      ],
      "metadata": {
        "id": "eeMrPu9lFPJp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "IDF (inverse document frequency) example\n",
        "The IDF (inverse document frequency) of a word is the measure of how significant that term is in the whole corpus (a body of documents).\n",
        "\n",
        "Let’s say the size of the corpus is 10,000,000 million documents. If we assume there are 0.3 million documents that contain the term “cat”, then the IDF (i.e. log {DF}) is given by the total number of documents (10,000,000) divided by the number of documents containing the term “cat” (300,000).\n",
        "\n",
        "IDF (cat) = log (10,000,000/300,000) = 1.52\n",
        "\n",
        "TF-IDF Calculation\n",
        "Put the TF and IDF calculations together to get a TF IDF score.\n",
        "\n",
        "∴ Wcat = (TF*IDF) cat = 0.12 * 1.52 = 0.182\n",
        "\n",
        "A TF-IDF score of 0.182 is much closer to 0 than 1. This suggests that “cat” is a common term with less weight. \n",
        "\n",
        "Now that you have this figured out (right?), let’s look at how this can benefit you."
      ],
      "metadata": {
        "id": "BYumj-R-Fb7q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input data: Each row is a bag of words with an ID\n",
        "df = spark.createDataFrame([\n",
        "    (0, \"PYTHON HIVE HIVE\".split(\" \")),\n",
        "    (1, \"JAVA JAVA SQL\".split(\" \"))\n",
        "], [\"id\", \"words\"])\n",
        "df.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2wjWIbEJC3-",
        "outputId": "4d7ac073-c62e-40b6-be57-6390bca7995f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "|id |words               |\n",
            "+---+--------------------+\n",
            "|0  |[PYTHON, HIVE, HIVE]|\n",
            "|1  |[JAVA, JAVA, SQL]   |\n",
            "+---+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, CountVectorizer will generate a vocabulary in case an a-priory vocabulary is not available. For instance, in this example CountVectorizer will create a vocabulary of size 4 which includes PYTHON, HIVE, JAVA and SQL terms. It will be followed by fitting of the CountVectorizer Model. During the fitting process, CountVectorizer will select the top VocabSize words ordered by term frequency. The model will produce a sparse vector which can be fed into other algorithms."
      ],
      "metadata": {
        "id": "vW1yqeo_JUqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit a CountVectorizerModel from the corpus\n",
        "from pyspark.ml.feature import CountVectorizer\n",
        "cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
        "model = cv.fit(df)\n",
        "result = model.transform(df)\n",
        "result.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kz3vgVTJM0C",
        "outputId": "6ba8fb85-8f3a-498c-9bf2-adf352155127"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+-------------------+\n",
            "|id |words               |features           |\n",
            "+---+--------------------+-------------------+\n",
            "|0  |[PYTHON, HIVE, HIVE]|(4,[0,3],[2.0,1.0])|\n",
            "|1  |[JAVA, JAVA, SQL]   |(4,[1,2],[2.0,1.0])|\n",
            "+---+--------------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*cE0fIDQbIWoHuFLcvujqmw.png'>\n",
        "\n",
        "\n",
        "For the purpose of understanding, the feature vector can be divided into 3 parts:\n",
        "\n",
        "<img src='https://miro.medium.com/max/828/1*LvsokJXq_I77pgKb9ZckUg.png'>\n",
        "\n",
        "* The leading number represents the size of the vector. Here, it is 4.\n",
        "* The first list of numbers represent the vector indices.\n",
        "\n",
        "<img src='https://miro.medium.com/max/640/1*BXTQ7nnRNtc9DLadnYeBAQ.png'>\n",
        "\n",
        "For instance, ‘JAVA’ term has a higher frequency of 2 as compared to term ‘SQL’ which has a frequency of 1. Therefore, ‘JAVA’ has index 1 whereas ‘SQL’ has index 2\n",
        "\n",
        "* The second list of numbers represents the values corresponding to these indices.\n",
        "It can be seen in document 2, ‘JAVA’ with index 1 has value 2 and ‘SQL’ with index 2 has value 1\n",
        "\n",
        "However, it should be noted that since the frequency of ‘HIVE’ and ‘JAVA’ is same, the indices are inter changeable.\n",
        "\n",
        "<img src='https://miro.medium.com/max/640/1*PdzU9Mp3-pC4baRyjK09Tw.png'>\n",
        "\n",
        "Here, ’HIVE’ has index 0 and ‘JAVA’ has index 1\n",
        "\n",
        "<img src='https://miro.medium.com/max/640/1*QuUdQrgdEv1fhKWMmDW9QA.png'>\n",
        "\n",
        "Here, ’HIVE’ has index 1 and ‘JAVA’ has index 0\n"
      ],
      "metadata": {
        "id": "yt4c_rleJdKg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HashingTF**\n",
        "\n",
        "HashingTF converts documents to vectors of fixed size. The default feature dimension is 262,144. The terms are mapped to indices using a Hash Function. The hash function used is MurmurHash 3. The term frequencies are computed with respect to the mapped indices."
      ],
      "metadata": {
        "id": "pf3hAOzCLAGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get term frequency vector through HashingTF\n",
        "from pyspark.ml.feature import HashingTF\n",
        "ht = HashingTF(inputCol=\"words\", outputCol=\"features\")\n",
        "result = ht.transform(df)\n",
        "result.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rb-1XarQK7c3",
        "outputId": "fb04e4e1-dac6-42ca-e9ac-72b76f9cfbd0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+----------------------------------+\n",
            "|id |words               |features                          |\n",
            "+---+--------------------+----------------------------------+\n",
            "|0  |[PYTHON, HIVE, HIVE]|(262144,[129668,191247],[2.0,1.0])|\n",
            "|1  |[JAVA, JAVA, SQL]   |(262144,[53343,256570],[2.0,1.0]) |\n",
            "+---+--------------------+----------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://miro.medium.com/max/828/1*ewi6TzvNlFGZgxIyY6U9BQ.png'>\n",
        "\n",
        "It can be seen in the above example that the dimension of the vector is set to default i.e. 262,144. Also, term ‘PYTHON’ is mapped to index 134160 by the hashing function and has frequency equal to 1. Similar, insights can be gained with respect to other terms."
      ],
      "metadata": {
        "id": "AZ0wEicHLYOA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ht2 = HashingTF(inputCol = 'words', outputCol='features', numFeatures=32) # 2 ^4\n",
        "result = ht2.transform(df)\n",
        "result.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DWA8JoKtleE",
        "outputId": "3115f36d-bb48-4955-ed2c-7cc08a911602"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+----------------------+\n",
            "|id |words               |features              |\n",
            "+---+--------------------+----------------------+\n",
            "|0  |[PYTHON, HIVE, HIVE]|(32,[4,15],[2.0,1.0]) |\n",
            "|1  |[JAVA, JAVA, SQL]   |(32,[26,31],[1.0,2.0])|\n",
            "+---+--------------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit the IDF data onto the datset\n",
        "\n",
        "idf = IDF(inputCol='features', outputCol= 'features 2')\n",
        "idfModel = idf.fit(result) # fit the model onto the result dataframe\n",
        "rescaledData = idfModel.transform(result)\n",
        "\n",
        "#show resulting data \n",
        "rescaledData.show(truncate = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3E55rkLOuvts",
        "outputId": "dfc63d52-3ee0-47b8-9edd-76014aa432eb"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+----------------------------------+----------------------------------------------------------------+\n",
            "|id |words               |features                          |features 2                                                      |\n",
            "+---+--------------------+----------------------------------+----------------------------------------------------------------+\n",
            "|0  |[PYTHON, HIVE, HIVE]|(262144,[129668,191247],[2.0,1.0])|(262144,[129668,191247],[0.8109302162163288,0.4054651081081644])|\n",
            "|1  |[JAVA, JAVA, SQL]   |(262144,[53343,256570],[2.0,1.0]) |(262144,[53343,256570],[0.8109302162163288,0.4054651081081644]) |\n",
            "+---+--------------------+----------------------------------+----------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example 2"
      ],
      "metadata": {
        "id": "zqnBGM9wLu6q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogdI6RYKqV_6",
        "outputId": "da6cd710-6ea7-438a-d648-b64685cc2957"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------------------+\n",
            "|id |words                            |\n",
            "+---+---------------------------------+\n",
            "|0  |The cow cow jumped and jumped cow|\n",
            "|1  |then the cow said                |\n",
            "|2  |I am a cow that jumped           |\n",
            "+---+---------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample DataFrame with repeating words\n",
        "dataframe = spark.createDataFrame([\n",
        "    (0, \"The cow cow jumped and jumped cow\"),\n",
        "    (1, \"then the cow said\"),\n",
        "    (2, \"I am a cow that jumped\")\n",
        "],[\"id\", \"words\"])\n",
        "\n",
        "dataframe.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3HSZu5z2qWAA",
        "outputId": "364e3491-13a9-49c3-ba74-0a9a1bf68e00"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------------------+-----------------------------------------+\n",
            "|id |words                            |tokens                                   |\n",
            "+---+---------------------------------+-----------------------------------------+\n",
            "|0  |The cow cow jumped and jumped cow|[the, cow, cow, jumped, and, jumped, cow]|\n",
            "|1  |then the cow said                |[then, the, cow, said]                   |\n",
            "|2  |I am a cow that jumped           |[i, am, a, cow, that, jumped]            |\n",
            "+---+---------------------------------+-----------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the words\n",
        "tokenizer = Tokenizer(inputCol=\"words\", outputCol=\"tokens\")\n",
        "wordsData = tokenizer.transform(dataframe)\n",
        "wordsData.show(truncate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use HashingTF to hash terms into fixed-length vectors, map to an index, and return a vector of term counts.\n",
        "\n",
        "* Note that HashingTF takes a numFeatures parameter that specifies the number of buckets into which the words will be split. This number must be higher than the number of unique words.\n",
        "\n",
        "* By default, this value is 2^18, or 262,144. We need to use a power of 2 so that indexes are evenly mapped."
      ],
      "metadata": {
        "id": "lpvLYLUaGIBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import CountVectorizer\n",
        "cV2 = CountVectorizer(inputCol = \"tokens\", outputCol = \"features\")\n",
        "model = cV2.fit(wordsData)\n",
        "result2 = model.transform(wordsData)\n",
        "result2.show(truncate = False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_WefivNxPZh",
        "outputId": "307c0fd4-debb-42a0-82f2-435e4fd809b5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------------------+-----------------------------------------+--------------------------------------------+\n",
            "|id |words                            |tokens                                   |features                                    |\n",
            "+---+---------------------------------+-----------------------------------------+--------------------------------------------+\n",
            "|0  |The cow cow jumped and jumped cow|[the, cow, cow, jumped, and, jumped, cow]|(10,[0,1,2,9],[3.0,2.0,1.0,1.0])            |\n",
            "|1  |then the cow said                |[then, the, cow, said]                   |(10,[0,2,7,8],[1.0,1.0,1.0,1.0])            |\n",
            "|2  |I am a cow that jumped           |[i, am, a, cow, that, jumped]            |(10,[0,1,3,4,5,6],[1.0,1.0,1.0,1.0,1.0,1.0])|\n",
            "+---+---------------------------------+-----------------------------------------+--------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "rOB8cgOzqWAE"
      },
      "outputs": [],
      "source": [
        "# Run the hashing term frequency\n",
        "hashing = HashingTF(inputCol=\"tokens\", outputCol=\"hashedValues\", numFeatures=pow(2,4))\n",
        "\n",
        "# Transform into a DF\n",
        "hashed_df = hashing.transform(wordsData)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw-AwFDTqWAJ",
        "outputId": "00a64e0b-673a-4e6b-d235-73021e95d3e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------------------------------+-----------------------------------------+----------------------------------------+\n",
            "|id |words                            |tokens                                   |hashedValues                            |\n",
            "+---+---------------------------------+-----------------------------------------+----------------------------------------+\n",
            "|0  |The cow cow jumped and jumped cow|[the, cow, cow, jumped, and, jumped, cow]|(16,[1,3,6,11],[1.0,3.0,2.0,1.0])       |\n",
            "|1  |then the cow said                |[then, the, cow, said]                   |(16,[0,1,3,13],[1.0,1.0,1.0,1.0])       |\n",
            "|2  |I am a cow that jumped           |[i, am, a, cow, that, jumped]            |(16,[0,3,6,12,13],[1.0,2.0,1.0,1.0,1.0])|\n",
            "+---+---------------------------------+-----------------------------------------+----------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display new DataFrame\n",
        "hashed_df.show(truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FWCmi9EXqWAO"
      },
      "outputs": [],
      "source": [
        "# Fit the IDF on the data set \n",
        "idf = IDF(inputCol=\"hashedValues\", outputCol=\"features\")\n",
        "idfModel = idf.fit(hashed_df)\n",
        "rescaledData = idfModel.transform(hashed_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j73AlD-SqWAW",
        "outputId": "c499cb5f-9b84-470e-94af-226562dc6072"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------------------------------------+-------------------------------------------------------------------------------------------------------+\n",
            "|tokens                                   |features                                                                                               |\n",
            "+-----------------------------------------+-------------------------------------------------------------------------------------------------------+\n",
            "|[the, cow, cow, jumped, and, jumped, cow]|(16,[1,3,6,11],[0.28768207245178085,0.0,0.5753641449035617,0.6931471805599453])                        |\n",
            "|[then, the, cow, said]                   |(16,[0,1,3,13],[0.28768207245178085,0.28768207245178085,0.0,0.28768207245178085])                      |\n",
            "|[i, am, a, cow, that, jumped]            |(16,[0,3,6,12,13],[0.28768207245178085,0.0,0.28768207245178085,0.6931471805599453,0.28768207245178085])|\n",
            "+-----------------------------------------+-------------------------------------------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Display the DataFrame\n",
        "rescaledData.select(\"tokens\", \"features\").show(truncate=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}