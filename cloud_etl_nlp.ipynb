{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maheenms/GoogleCoLab/blob/main/cloud_etl_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YV4ajtm-iijA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.2  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "spark_version = 'spark-3.2.2'\n",
        "#spark_version = 'spark-<enter version>'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "21VIzGHIioxp"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CloudETLProject\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGQc0BlXit96",
        "outputId": "82c5710d-d1b9-4c61-ee65-90505f95880c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+--------------------+------+-----------------+-----------+\n",
            "|                 _c0|            drugName|           condition|              review|rating|             date|usefulCount|\n",
            "+--------------------+--------------------+--------------------+--------------------+------+-----------------+-----------+\n",
            "|              206461|           Valsartan|Left Ventricular ...|\"\"\"It has no side...|   9.0|     May 20, 2012|         27|\n",
            "|               95260|          Guanfacine|                ADHD|\"\"\"My son is half...|  null|             null|       null|\n",
            "|We have tried man...|                 8.0|      April 27, 2010|                 192|  null|             null|       null|\n",
            "|               92703|              Lybrel|       Birth Control|\"\"\"I used to take...|  null|             null|       null|\n",
            "|The positive side...|                 5.0|   December 14, 2009|                  17|  null|             null|       null|\n",
            "|              138000|          Ortho Evra|       Birth Control|\"\"\"This is my fir...|   8.0| November 3, 2015|         10|\n",
            "|               35696|Buprenorphine / n...|   Opiate Dependence|\"\"\"Suboxone has c...|   9.0|November 27, 2016|         37|\n",
            "|              155963|              Cialis|Benign Prostatic ...|\"\"\"2nd day on 5mg...|   2.0|November 28, 2015|         43|\n",
            "|              165907|      Levonorgestrel|Emergency Contrac...|\"\"\"He pulled out,...|   1.0|    March 7, 2017|          5|\n",
            "|              102654|        Aripiprazole|     Bipolar Disorde|\"\"\"Abilify change...|  10.0|   March 14, 2015|         32|\n",
            "+--------------------+--------------------+--------------------+--------------------+------+-----------------+-----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark import SparkFiles\n",
        "# Load in data into a DataFrame\n",
        "\n",
        "url = \"/content/drugsComTrain_raw.tsv\" #enter correct address here\n",
        "#spark.sparkContext.addFile(url) No need for sparkcontext if read from google drive\n",
        "\n",
        "df = spark.read.option('header', 'true').csv(SparkFiles.get(url), inferSchema=True, sep='\\t', timestampFormat=\"mm/dd/yy\")\n",
        "df.show(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHrs8Uk1i0RX"
      },
      "source": [
        "## Transform DataFrame to fit review_rating table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvpPZ1TEiwyu",
        "outputId": "b2af2c78-e6c4-4406-9800-ce5a4468b727"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------+-----------------+\n",
            "|              review|rating|             date|\n",
            "+--------------------+------+-----------------+\n",
            "|\"\"\"It has no side...|   9.0|     May 20, 2012|\n",
            "|\"\"\"My son is half...|  null|             null|\n",
            "|                 192|  null|             null|\n",
            "|\"\"\"I used to take...|  null|             null|\n",
            "|                  17|  null|             null|\n",
            "|\"\"\"This is my fir...|   8.0| November 3, 2015|\n",
            "|\"\"\"Suboxone has c...|   9.0|November 27, 2016|\n",
            "|\"\"\"2nd day on 5mg...|   2.0|November 28, 2015|\n",
            "|\"\"\"He pulled out,...|   1.0|    March 7, 2017|\n",
            "|\"\"\"Abilify change...|  10.0|   March 14, 2015|\n",
            "|\"\"\" I Ve had  not...|   1.0|   August 9, 2016|\n",
            "|\"\"\"I had been on ...|   8.0| December 8, 2016|\n",
            "|\"\"\"I have been on...|   9.0|  January 1, 2015|\n",
            "|\"\"\"I have taken a...|  null|             null|\n",
            "|                null|  null|             null|\n",
            "|                  54|  null|             null|\n",
            "|\"\"\"I had Crohn&#0...|   4.0|     July 6, 2013|\n",
            "|\"\"\"Have a little ...|   4.0|September 7, 2017|\n",
            "|\"\"\"Started Nexpla...|  null|             null|\n",
            "|                  10|  null|             null|\n",
            "+--------------------+------+-----------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "review_df = df.select([\"review\",\"rating\", \"date\"])\n",
        "review_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWBjwNJqzr5l",
        "outputId": "f2fd6a9b-6b02-44ae-a72a-62c469536888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------------------+--------------------+-------------+\n",
            "|label|              date|              review|review_length|\n",
            "+-----+------------------+--------------------+-------------+\n",
            "|  9.0|      May 20, 2012|\"\"\"It has no side...|           83|\n",
            "|  8.0|  November 3, 2015|\"\"\"This is my fir...|          452|\n",
            "|  9.0| November 27, 2016|\"\"\"Suboxone has c...|          723|\n",
            "|  2.0| November 28, 2015|\"\"\"2nd day on 5mg...|          407|\n",
            "|  1.0|     March 7, 2017|\"\"\"He pulled out,...|          146|\n",
            "| 10.0|    March 14, 2015|\"\"\"Abilify change...|          737|\n",
            "|  1.0|    August 9, 2016|\"\"\" I Ve had  not...|          197|\n",
            "|  8.0|  December 8, 2016|\"\"\"I had been on ...|          741|\n",
            "|  9.0|   January 1, 2015|\"\"\"I have been on...|          734|\n",
            "|  4.0|      July 6, 2013|\"\"\"I had Crohn&#0...|          407|\n",
            "|  4.0| September 7, 2017|\"\"\"Have a little ...|          595|\n",
            "|  9.0|  January 19, 2017|\"\"\"I have been ta...|          737|\n",
            "|  9.0|September 22, 2017|\"\"\"This drug work...|          680|\n",
            "|  9.0|    March 15, 2017|\"\"\"I&#039;ve been...|          715|\n",
            "| 10.0|  November 9, 2014|\"\"\"I&#039;ve been...|          774|\n",
            "| 10.0| September 1, 2015|\"\"\"I have been on...|          484|\n",
            "|  8.0|      July 9, 2010|\"\"\"Spring of 2008...|          528|\n",
            "| 10.0|     April 3, 2016|\"\"\"I have insomni...|          761|\n",
            "|  9.0|   August 11, 2014|\"\"\"Nexplanon does...|          430|\n",
            "| 10.0|September 16, 2017|\"\"\"I live in West...|          762|\n",
            "+-----+------------------+--------------------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import regexp_extract, length\n",
        "review_df = df.withColumnRenamed(\"rating\", \"label\").select([\"label\", \"date\", \"review\"])\n",
        "review_df = review_df.withColumn('review_length', length(review_df['review'])).dropna()\n",
        "review_df.cache()\n",
        "review_df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nRtLVc7i8gK"
      },
      "source": [
        "## Create Data Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "DPFN7XcGiwtP"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "# Create all the features to the data set\n",
        "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"token_text\")\n",
        "stopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\n",
        "hashingTF = HashingTF(inputCol=\"token_text\", outputCol='hash_token')\n",
        "idf = IDF(inputCol='hash_token', outputCol='idf_token')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "CxsBBQRiiwk-"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.ml.linalg import Vector\n",
        "\n",
        "# Create feature vectors (merge idf_token and review_length)\n",
        "clean_up = VectorAssembler(inputCols=['idf_token', 'review_length'], outputCol='features')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nQ6f6eXfjDfD"
      },
      "outputs": [],
      "source": [
        "# Create and run a data processing Pipeline\n",
        "from pyspark.ml import Pipeline\n",
        "data_prep_pipeline = Pipeline(stages=[tokenizer, stopremove, hashingTF, idf, clean_up])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_ZkNHtAjGUS"
      },
      "source": [
        "## Transform DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Fbq2d16jjDcQ"
      },
      "outputs": [],
      "source": [
        "# Fit and transform the pipeline\n",
        "cleaner = data_prep_pipeline.fit(review_df)\n",
        "cleaned = cleaner.transform(review_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAIOltOojKal",
        "outputId": "f3cdb69a-a952-4ccf-ed0d-d4fa8ca0b280"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  9.0|(262145,[2325,623...|\n",
            "|  8.0|(262145,[8254,190...|\n",
            "|  9.0|(262145,[2306,581...|\n",
            "|  2.0|(262145,[1091,154...|\n",
            "|  1.0|(262145,[19036,33...|\n",
            "| 10.0|(262145,[1091,188...|\n",
            "|  1.0|(262145,[19036,32...|\n",
            "|  8.0|(262145,[1546,140...|\n",
            "|  9.0|(262145,[365,1603...|\n",
            "|  4.0|(262145,[10353,19...|\n",
            "|  4.0|(262145,[1578,160...|\n",
            "|  9.0|(262145,[2050,232...|\n",
            "|  9.0|(262145,[1578,205...|\n",
            "|  9.0|(262145,[9420,125...|\n",
            "| 10.0|(262145,[12175,12...|\n",
            "| 10.0|(262145,[1578,196...|\n",
            "|  8.0|(262145,[432,1097...|\n",
            "| 10.0|(262145,[1303,942...|\n",
            "|  9.0|(262145,[1546,232...|\n",
            "| 10.0|(262145,[2284,447...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Show label of ham spam and resulting features\n",
        "cleaned.select(['label', 'features']).show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S37P7oA2jMF9"
      },
      "source": [
        "## Run NaiveBayes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2_KHNZkqjN-Z"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "# Break data down into a training set and a testing set\n",
        "training, testing = cleaned.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Create a Naive Bayes model and fit training data\n",
        "nb = NaiveBayes() #labelCol='label', featuresCol='features'\n",
        "predictor = nb.fit(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGP2q-_D2one",
        "outputId": "a8f554f6-ec57-4bed-ba76-76eeb7c992d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|label|         date|              review|review_length|          token_text|         stop_tokens|          hash_token|           idf_token|            features|\n",
            "+-----+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "|  1.0|April 1, 2009|\"\"\"It made me ext...|           87|[\"\"\"it, made, me,...|[\"\"\"it, made, ext...|(262144,[19923,30...|(262144,[19923,30...|(262145,[19923,30...|\n",
            "|  1.0|April 1, 2009|\"\"\"It made me ext...|           87|[\"\"\"it, made, me,...|[\"\"\"it, made, ext...|(262144,[19923,30...|(262144,[19923,30...|(262145,[19923,30...|\n",
            "|  1.0|April 1, 2015|\"\"\"I came down wi...|          349|[\"\"\"i, came, down...|[\"\"\"i, came, flu,...|(262144,[5381,942...|(262144,[5381,942...|(262145,[5381,942...|\n",
            "|  1.0|April 1, 2015|\"\"\"I have been on...|          375|[\"\"\"i, have, been...|[\"\"\"i, victoza, l...|(262144,[11104,19...|(262144,[11104,19...|(262145,[11104,19...|\n",
            "|  1.0|April 1, 2016|\"\"\"Anaphylaxis re...|           87|[\"\"\"anaphylaxis, ...|[\"\"\"anaphylaxis, ...|(262144,[10235,61...|(262144,[10235,61...|(262145,[10235,61...|\n",
            "|  1.0|April 1, 2016|\"\"\"DO NOT SWITCH!...|          746|[\"\"\"do, not, swit...|[\"\"\"do, switch!, ...|(262144,[2306,232...|(262144,[2306,232...|(262145,[2306,232...|\n",
            "|  1.0|April 1, 2016|\"\"\"Headache.pain ...|           70|[\"\"\"headache.pain...|[\"\"\"headache.pain...|(262144,[10890,44...|(262144,[10890,44...|(262145,[10890,44...|\n",
            "|  1.0|April 1, 2016|\"\"\"I began Zarah ...|         1491|[\"\"\"i, began, zar...|[\"\"\"i, began, zar...|(262144,[1603,233...|(262144,[1603,233...|(262145,[1603,233...|\n",
            "|  1.0|April 1, 2016|\"\"\"I had been on ...|          686|[\"\"\"i, had, been,...|[\"\"\"i, different,...|(262144,[1546,232...|(262144,[1546,232...|(262145,[1546,232...|\n",
            "|  1.0|April 1, 2016|\"\"\"I was constipa...|          548|[\"\"\"i, was, const...|[\"\"\"i, constipate...|(262144,[1696,232...|(262144,[1696,232...|(262145,[1696,232...|\n",
            "|  1.0|April 1, 2016|\"\"\"I&#039;ve been...|          604|[\"\"\"i&#039;ve, be...|[\"\"\"i&#039;ve, or...|(262144,[1603,145...|(262144,[1603,145...|(262145,[1603,145...|\n",
            "|  1.0|April 1, 2016|\"\"\"I&#039;ve been...|          604|[\"\"\"i&#039;ve, be...|[\"\"\"i&#039;ve, or...|(262144,[1603,145...|(262144,[1603,145...|(262145,[1603,145...|\n",
            "|  1.0|April 1, 2016|\"\"\"I&#039;ve been...|          185|[\"\"\"i&#039;ve, be...|[\"\"\"i&#039;ve, tw...|(262144,[9056,218...|(262144,[9056,218...|(262145,[9056,218...|\n",
            "|  1.0|April 1, 2016|\"\"\"In my experien...|          523|[\"\"\"in, my, exper...|[\"\"\"in, experienc...|(262144,[1603,111...|(262144,[1603,111...|(262145,[1603,111...|\n",
            "|  1.0|April 1, 2016|\"\"\"Picked up moni...|          438|[\"\"\"picked, up, m...|[\"\"\"picked, monis...|(262144,[19036,21...|(262144,[19036,21...|(262145,[19036,21...|\n",
            "|  1.0|April 1, 2017|\"\"\"I started sert...|          129|[\"\"\"i, started, s...|[\"\"\"i, started, s...|(262144,[24698,32...|(262144,[24698,32...|(262145,[24698,32...|\n",
            "|  1.0|April 1, 2017|\"\"\"I was prescrib...|          649|[\"\"\"i, was, presc...|[\"\"\"i, prescribed...|(262144,[1797,942...|(262144,[1797,942...|(262145,[1797,942...|\n",
            "|  1.0|April 1, 2017|\"\"\"I&#039;m so ma...|          305|[\"\"\"i&#039;m, so,...|[\"\"\"i&#039;m, mad...|(262144,[19036,27...|(262144,[19036,27...|(262145,[19036,27...|\n",
            "|  1.0|April 1, 2017|\"\"\"This by far ha...|          466|[\"\"\"this, by, far...|[\"\"\"this, far, on...|(262144,[365,3283...|(262144,[365,3283...|(262145,[365,3283...|\n",
            "|  1.0|April 1, 2017|\"\"\"been on Hydrox...|           79|[\"\"\"been, on, hyd...|[\"\"\"been, hydroxy...|(262144,[23188,30...|(262144,[23188,30...|(262145,[23188,30...|\n",
            "+-----+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "training.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSO42Oc9jPRV",
        "outputId": "9eee64f7-e2e9-4b76-e03d-8d31e0c5c479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|label|         date|              review|review_length|          token_text|         stop_tokens|          hash_token|           idf_token|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|  1.0|April 1, 2008|\"\"\"this medicine ...|          118|[\"\"\"this, medicin...|[\"\"\"this, medicin...|(262144,[71929,86...|(262144,[71929,86...|(262145,[71929,86...|[-657.90224497362...|[0.99999999999998...|       0.0|\n",
            "|  1.0|April 1, 2014|\"\"\"I have bronchi...|          430|[\"\"\"i, have, bron...|[\"\"\"i, bronchial,...|(262144,[12098,15...|(262144,[12098,15...|(262145,[12098,15...|[-2733.2337514288...|[2.15760001465185...|       9.0|\n",
            "|  1.0|April 1, 2015|\"\"\"I came down wi...|          349|[\"\"\"i, came, down...|[\"\"\"i, came, flu,...|(262144,[5381,942...|(262144,[5381,942...|(262145,[5381,942...|[-1938.8661503739...|[0.99999999642236...|       0.0|\n",
            "|  1.0|April 1, 2016|\"\"\"I had been on ...|          686|[\"\"\"i, had, been,...|[\"\"\"i, different,...|(262144,[1546,232...|(262144,[1546,232...|(262145,[1546,232...|[-3167.7204539380...|[1.0,6.7041681272...|       0.0|\n",
            "|  1.0|April 1, 2016|\"\"\"I&#039;ve been...|          269|[\"\"\"i&#039;ve, be...|[\"\"\"i&#039;ve, pi...|(262144,[1546,187...|(262144,[1546,187...|(262145,[1546,187...|[-994.66561454245...|[0.00387231268584...|       6.0|\n",
            "+-----+-------------+--------------------+-------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Tranform the model with the testing data\n",
        "test_results = predictor.transform(testing)\n",
        "test_results.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tik9-ZskjPvO"
      },
      "source": [
        "## Predict accuracy of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuFyog8-jQCQ",
        "outputId": "e182239f-4071-4114-9a6a-1f3e5a61ed79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of model at predicting reviews was: 0.067020\n"
          ]
        }
      ],
      "source": [
        "# Use the Class Evaluator for a cleaner description\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "acc_eval = MulticlassClassificationEvaluator()\n",
        "acc = acc_eval.evaluate(test_results)\n",
        "print(\"Accuracy of model at predicting reviews was: %f\" % acc)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}